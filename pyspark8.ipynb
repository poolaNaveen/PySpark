{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOVROxMRKuBs1SXe103fOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poolaNaveen/PySpark/blob/main/pyspark8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating RDD's"
      ],
      "metadata": {
        "id": "VwoS_GWlVe6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query 1\n"
      ],
      "metadata": {
        "id": "ikjJsF7CXEvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('rdd_test').getOrCreate()\n",
        "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
        "print(rdd.collect())\n",
        "# Paralleize: convert a python list into an RDD that spark can distribute across clusters.\n",
        "#collect() Brings all RDD data bavk to the driver."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-XgNjc9VeMw",
        "outputId": "7eafc0f0-c456-4a6f-d00e-884b22e74d27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 2\n"
      ],
      "metadata": {
        "id": "aai2QRXyX8k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('rdd').getOrCreate()\n",
        "rdd1 = spark.sparkContext.parallelize([1,2,3,4,5,5,6])\n",
        "print(rdd1.collect())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSGPJOukYWKh",
        "outputId": "29a345b2-1a74-4e8f-f4f9-54ce66f80956"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 3\n"
      ],
      "metadata": {
        "id": "UoM5p7S3Zw2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8])\n",
        "result = rdd2.map(lambda x: x*2)\n",
        "print(result.collect())\n",
        "# map()applies the function x * 2 to every element in the RDD."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYfsY-ALVeIW",
        "outputId": "7e3496fb-83e6-4141-fbc2-34c3ef53cfcb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10, 12, 14, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3 = spark.sparkContext.parallelize([1,3,4,6,6,7,8,9])\n",
        "result = rdd3.map(lambda x: x* 2)\n",
        "print(result.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_A_wsVMY4L-",
        "outputId": "8a0d688d-e0e1-4a7e-93e5-03addc0b7f08"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 6, 8, 12, 12, 14, 16, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd4 = spark.sparkContext.parallelize([2,4,6,8,10])\n",
        "result = rdd4.map(lambda x: x*2)\n",
        "print(result.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpBevPJQVduB",
        "outputId": "6f45e8e7-287d-4374-8e20-467b33e5e9b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 8, 12, 16, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_zwN9z5VYkF",
        "outputId": "435bbdc4-74df-4dab-ade2-106dafe0c370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 8, 10]\n"
          ]
        }
      ],
      "source": [
        "rdd5 = spark.sparkContext.parallelize([2,4,5,8,10])\n",
        "even = rdd5.filter(lambda x: x % 2==0)\n",
        "print(even.collect())\n",
        "#Filter() keeps only those elements that \"True\" in the condition."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([2,4,5,6,7,7,8,9,10])\n",
        "result = rdd.filter(lambda x: x%2 == 0)\n",
        "print(result.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u9fxEDMdo73",
        "outputId": "274315d2-b404-4fe2-bccd-4679c689906d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    }
  ]
}