{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm60kejD1FBcW1aHGokC27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poolaNaveen/PySpark/blob/main/pyspark6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BRONZE, SILVER, GOLD**"
      ],
      "metadata": {
        "id": "1MUARKkppCLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "K0amt-gspXUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxM08cOpmPjM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW4iKnn8mW7a",
        "outputId": "c53ce24c-f885-47a7-9c2b-843db665e0d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyspark 3.5.0\n",
            "Uninstalling pyspark-3.5.0:\n",
            "  Successfully uninstalled pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.1 delta-spark==3.2.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzAEqnXfmW5G",
        "outputId": "ef4c22a1-1182-4f9f-845f-a12deb5ffad2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: delta-spark==3.2.0 in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from delta-spark==3.2.0) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.0.0->delta-spark==3.2.0) (3.23.0)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=e29d6b3d1c521809e0783194a0c6d67f199d592aa57748d771b85f05b6126497\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/91/5f/283b53010a8016a4ff1c4a1edd99bbe73afacb099645b5471b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"citibike\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ],
      "metadata": {
        "id": "OS15yTbTmW2u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"DeltaTest\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "# Create a small DataFrame\n",
        "df = spark.createDataFrame([(1, \"bikeA\"), (2, \"bikeB\")], [\"id\", \"name\"])\n",
        "\n",
        "# Write to Delta\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/test_delta\")\n",
        "\n",
        "# Read back\n",
        "df2 = spark.read.format(\"delta\").load(\"/content/test_delta\")\n",
        "df2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaiA6DQTmWte",
        "outputId": "da38c451-63a2-40a7-f219-32054ed57828"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|  2|bikeB|\n",
            "|  1|bikeA|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, trim, current_timestamp\n",
        "from pyspark.sql.types import DecimalType, TimestampType\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "# 4. Configure Spark with Delta Lake\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"citibike\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "# 5. Read Landing Layer CSV\n",
        "landing_df = spark.read.csv(\"/content/travel.csv\",\n",
        "                            header=True,\n",
        "                            inferSchema=True)\n",
        "\n",
        "# 6. Basic cleaning\n",
        "clean_df = landing_df.select(\n",
        "    trim(col(\"ride_id\")).alias(\"ride_id\"),\n",
        "    trim(col(\"rideable_type\")).alias(\"rideable_type\"),\n",
        "    trim(col(\"started_at\")).alias(\"started_at\"),\n",
        "    trim(col(\"ended_at\")).alias(\"ended_at\"),\n",
        "    trim(col(\"start_station_name\")).alias(\"start_station_name\"),\n",
        "    trim(col(\"start_station_id\")).alias(\"start_station_id\"),\n",
        "    trim(col(\"end_station_name\")).alias(\"end_station_name\"),\n",
        "    trim(col(\"end_station_id\")).alias(\"end_station_id\"),\n",
        "    col(\"start_lat\").cast(DecimalType(10, 6)),\n",
        "    col(\"start_lng\").cast(DecimalType(10, 6)),\n",
        "    col(\"end_lat\").cast(DecimalType(10, 6)),\n",
        "    col(\"end_lng\").cast(DecimalType(10, 6)),\n",
        "    trim(col(\"member_casual\")).alias(\"member_casual\")\n",
        ")\n",
        "\n",
        "# 7. Convert timestamps and add metadata\n",
        "bronze_df = clean_df.withColumn(\"started_at\", col(\"started_at\").cast(TimestampType())) \\\n",
        "                    .withColumn(\"ended_at\", col(\"ended_at\").cast(TimestampType())) \\\n",
        "                    .withColumn(\"metadata\", current_timestamp())\n",
        "\n",
        "# 8. Save as Delta Lake Bronze Table\n",
        "bronze_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/content/bronze/jc_citibike\")\n",
        "\n",
        "# 9. Read back to verify\n",
        "df = spark.read.format(\"delta\").load(\"/content/bronze/jc_citibike\")\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dubuD1zonCm",
        "outputId": "3d50597b-545d-4176-fbe6-52c6e83ef027"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+--------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+--------------------+\n",
            "|         ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|    end_station_name|end_station_id|start_lat| start_lng|  end_lat|   end_lng|member_casual|            metadata|\n",
            "+----------------+-------------+----------+--------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+--------------------+\n",
            "|29DAF43DD84B4B7A|electric_bike|      NULL|    NULL|   6 St & Grand St|           HB302|Mama Johnson Fiel...|         HB404|40.744398|-74.034501|40.743140|-74.040041|       member|2025-12-02 15:16:...|\n",
            "|B11B4220F7195025|electric_bike|      NULL|    NULL|  Heights Elevator|           JC059|        Jersey & 3rd|         JC074|40.748716|-74.040443|40.723332|-74.045953|       member|2025-12-02 15:16:...|\n",
            "|18D5B30305F602B9|electric_bike|      NULL|    NULL|      Jersey & 3rd|           JC074|       Hamilton Park|         JC009|40.723332|-74.045953|40.727596|-74.044247|       member|2025-12-02 15:16:...|\n",
            "|532EB2D9DB68567D|electric_bike|      NULL|    NULL|      Jersey & 3rd|           JC074|     Jersey & 6th St|         JC027|40.723332|-74.045953|40.725289|-74.045572|       member|2025-12-02 15:16:...|\n",
            "|EA7C9C945D7D57AA|electric_bike|      NULL|    NULL|   6 St & Grand St|           HB302|   Madison St & 1 St|         HB402|40.744398|-74.034501|40.738790|-74.039300|       member|2025-12-02 15:16:...|\n",
            "+----------------+-------------+----------+--------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 1. Read Bronze Delta Table\n",
        "bronze_df = spark.read.format(\"delta\").load(\"/content/bronze/jc_citibike\")\n",
        "\n",
        "# 2. Enrich data for Silver layer\n",
        "silver_df = bronze_df \\\n",
        "    .withColumn(\"ride_duration_minutes\",\n",
        "                (F.col(\"ended_at\").cast(\"long\") - F.col(\"started_at\").cast(\"long\")) / 60) \\\n",
        "    .withColumn(\"member_type\",\n",
        "                F.when(F.col(\"member_casual\") == \"member\", \"Member\")\n",
        "                 .when(F.col(\"member_casual\") == \"casual\", \"Casual\")\n",
        "                 .otherwise(\"Unknown\")) \\\n",
        "    .filter(F.col(\"started_at\").isNotNull() & F.col(\"ended_at\").isNotNull()) \\\n",
        "    .filter(F.col(\"ride_duration_minutes\") > 0)\n",
        "\n",
        "# 3. Save Silver Delta Table\n",
        "silver_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/content/silver/jc_citibike\")\n",
        "\n",
        "# 4. Read back to verify\n",
        "df_silver = spark.read.format(\"delta\").load(\"/content/silver/jc_citibike\")\n",
        "df_silver.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRpoAgC3poGK",
        "outputId": "12831d47-47b0-4714-eb95-1ed8c33d179a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+--------+---------------------+-----------+\n",
            "|ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|end_station_name|end_station_id|start_lat|start_lng|end_lat|end_lng|member_casual|metadata|ride_duration_minutes|member_type|\n",
            "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+--------+---------------------+-----------+\n",
            "+-------+-------------+----------+--------+------------------+----------------+----------------+--------------+---------+---------+-------+-------+-------------+--------+---------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 1. Read Silver Delta Table\n",
        "silver_df = spark.read.format(\"delta\").load(\"/content/silver/jc_citibike\")\n",
        "\n",
        "# 2. Aggregate for Gold layer\n",
        "gold_df = silver_df.groupBy(\n",
        "    F.date_format(\"started_at\", \"yyyy-MM-dd\").alias(\"ride_date\"),\n",
        "    \"member_type\",\n",
        "    \"start_station_name\"\n",
        ").agg(\n",
        "    F.count(\"*\").alias(\"total_rides\"),\n",
        "    F.avg(\"ride_duration_minutes\").alias(\"avg_duration_minutes\"),\n",
        "    F.min(\"ride_duration_minutes\").alias(\"min_duration_minutes\"),\n",
        "    F.max(\"ride_duration_minutes\").alias(\"max_duration_minutes\")\n",
        ")\n",
        "\n",
        "# 3. Save Gold Delta Table\n",
        "gold_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/content/gold/jc_citibike\")\n",
        "\n",
        "# 4. Read back to verify\n",
        "df_gold = spark.read.format(\"delta\").load(\"/content/gold/jc_citibike\")\n",
        "df_gold.show(30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdTBcw49qADU",
        "outputId": "947c11ec-da64-4c28-da85-14b2395b1ff4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+------------------+-----------+--------------------+--------------------+--------------------+\n",
            "|ride_date|member_type|start_station_name|total_rides|avg_duration_minutes|min_duration_minutes|max_duration_minutes|\n",
            "+---------+-----------+------------------+-----------+--------------------+--------------------+--------------------+\n",
            "+---------+-----------+------------------+-----------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "landing_df = spark.read.csv(\"/content/travel.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "qkp1-UCGq9Ef"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, trim, current_timestamp\n",
        "from pyspark.sql.types import DecimalType, TimestampType\n",
        "\n",
        "bronze_df = landing_df.select(\n",
        "    trim(col(\"ride_id\")).alias(\"ride_id\"),\n",
        "    trim(col(\"rideable_type\")).alias(\"rideable_type\"),\n",
        "    trim(col(\"started_at\")).cast(TimestampType()).alias(\"started_at\"),\n",
        "    trim(col(\"ended_at\")).cast(TimestampType()).alias(\"ended_at\"),\n",
        "    trim(col(\"start_station_name\")).alias(\"start_station_name\"),\n",
        "    trim(col(\"start_station_id\")).alias(\"start_station_id\"),\n",
        "    trim(col(\"end_station_name\")).alias(\"end_station_name\"),\n",
        "    trim(col(\"end_station_id\")).alias(\"end_station_id\"),\n",
        "    col(\"start_lat\").cast(DecimalType(10, 6)),\n",
        "    col(\"start_lng\").cast(DecimalType(10, 6)),\n",
        "    col(\"end_lat\").cast(DecimalType(10, 6)),\n",
        "    col(\"end_lng\").cast(DecimalType(10, 6)),\n",
        "    trim(col(\"member_casual\")).alias(\"member_casual\")\n",
        ").withColumn(\"metadata\", current_timestamp())\n",
        "bronze_df.show(10)\n",
        "\n",
        "bronze_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/bronze/jc_citibike\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkI3Z8mvrJOb",
        "outputId": "ec12b05d-3018-4ad9-e098-b90431ee911b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+----------+--------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+--------------------+\n",
            "|         ride_id|rideable_type|started_at|ended_at|start_station_name|start_station_id|    end_station_name|end_station_id|start_lat| start_lng|  end_lat|   end_lng|member_casual|            metadata|\n",
            "+----------------+-------------+----------+--------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+--------------------+\n",
            "|29DAF43DD84B4B7A|electric_bike|      NULL|    NULL|   6 St & Grand St|           HB302|Mama Johnson Fiel...|         HB404|40.744398|-74.034501|40.743140|-74.040041|       member|2025-12-02 15:28:...|\n",
            "|B11B4220F7195025|electric_bike|      NULL|    NULL|  Heights Elevator|           JC059|        Jersey & 3rd|         JC074|40.748716|-74.040443|40.723332|-74.045953|       member|2025-12-02 15:28:...|\n",
            "|18D5B30305F602B9|electric_bike|      NULL|    NULL|      Jersey & 3rd|           JC074|       Hamilton Park|         JC009|40.723332|-74.045953|40.727596|-74.044247|       member|2025-12-02 15:28:...|\n",
            "|532EB2D9DB68567D|electric_bike|      NULL|    NULL|      Jersey & 3rd|           JC074|     Jersey & 6th St|         JC027|40.723332|-74.045953|40.725289|-74.045572|       member|2025-12-02 15:28:...|\n",
            "|EA7C9C945D7D57AA|electric_bike|      NULL|    NULL|   6 St & Grand St|           HB302|   Madison St & 1 St|         HB402|40.744398|-74.034501|40.738790|-74.039300|       member|2025-12-02 15:28:...|\n",
            "|DA232FF47222E86C| classic_bike|      NULL|    NULL|   6 St & Grand St|           HB302|   Madison St & 1 St|         HB402|40.744398|-74.034501|40.738790|-74.039300|       member|2025-12-02 15:28:...|\n",
            "|416547516DE5132F|electric_bike|      NULL|    NULL|           Hilltop|           JC019| Leonard Gordon Park|         JC080|40.731169|-74.057574|40.745910|-74.057271|       member|2025-12-02 15:28:...|\n",
            "|E25EDA33910F90F0|electric_bike|      NULL|    NULL|           Hilltop|           JC019| Leonard Gordon Park|         JC080|40.731169|-74.057574|40.745910|-74.057271|       member|2025-12-02 15:28:...|\n",
            "|D209FF2521E26D16| classic_bike|      NULL|    NULL|    Jackson Square|           JC063|          Bergen Ave|         JC095|40.711130|-74.078900|40.722104|-74.071455|       member|2025-12-02 15:28:...|\n",
            "|BC9F0D06A5AFF751|electric_bike|      NULL|    NULL|   6 St & Grand St|           HB302|Southwest Park - ...|         HB401|40.744398|-74.034501|40.737551|-74.041664|       member|2025-12-02 15:28:...|\n",
            "+----------------+-------------+----------+--------+------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df = spark.read.format(\"delta\").load(\"/content/bronze/jc_citibike\")\n",
        "bronze_df.select(\"ride_id\",\"started_at\",\"ended_at\").show(10, False)\n",
        "silver_df = bronze_df.select(\n",
        "    col(\"ride_id\"),\n",
        "    to_date(col(\"started_at\")).alias(\"trip_start_date\"),\n",
        "    col(\"started_at\"),\n",
        "    col(\"ended_at\"),\n",
        "    col(\"start_station_name\"),\n",
        "    col(\"end_station_name\"),\n",
        "    ((col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\")) / 60).alias(\"trip_duration_mins\"),\n",
        "    col(\"metadata\")\n",
        ")\n",
        "\n",
        "silver_df.show(10, False)\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "bronze_df = landing_df.select(\n",
        "    col(\"ride_id\"),\n",
        "    to_timestamp(col(\"started_at\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"started_at\"),\n",
        "    to_timestamp(col(\"ended_at\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"ended_at\"),\n",
        "    # ... other columns ...\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bc5S4vqv228",
        "outputId": "6a0d395c-c9f9-4085-abed-498629e98850"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------+--------+\n",
            "|ride_id         |started_at|ended_at|\n",
            "+----------------+----------+--------+\n",
            "|DDD973FEED07C0DF|NULL      |NULL    |\n",
            "|7356497BFB2EDA2F|NULL      |NULL    |\n",
            "|10743EF6640BBB15|NULL      |NULL    |\n",
            "|8FC91C5AC697CB78|NULL      |NULL    |\n",
            "|B600C6359586E0E8|NULL      |NULL    |\n",
            "|FA1DDE4B52ADBF5A|NULL      |NULL    |\n",
            "|F41D88A622E8050C|NULL      |NULL    |\n",
            "|851A4ED5495DE630|NULL      |NULL    |\n",
            "|23165E2C35306B50|NULL      |NULL    |\n",
            "|99D3D0B592E30B99|NULL      |NULL    |\n",
            "+----------------+----------+--------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------------+---------------+----------+--------+----------------------------------------+------------------------------------------+------------------+--------------------------+\n",
            "|ride_id         |trip_start_date|started_at|ended_at|start_station_name                      |end_station_name                          |trip_duration_mins|metadata                  |\n",
            "+----------------+---------------+----------+--------+----------------------------------------+------------------------------------------+------------------+--------------------------+\n",
            "|DDD973FEED07C0DF|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|7356497BFB2EDA2F|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|10743EF6640BBB15|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|8FC91C5AC697CB78|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|B600C6359586E0E8|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|FA1DDE4B52ADBF5A|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|F41D88A622E8050C|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|851A4ED5495DE630|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|23165E2C35306B50|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "|99D3D0B592E30B99|NULL           |NULL      |NULL    |Hoboken Terminal - Hudson St & Hudson Pl|Southwest Park - Jackson St & Observer Hwy|NULL              |2025-12-02 15:28:42.001463|\n",
            "+----------------+---------------+----------+--------+----------------------------------------+------------------------------------------+------------------+--------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.1 delta-spark==3.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IWFR1QlPmrU",
        "outputId": "c3625622-c6eb-4ad0-d322-f31c1834a8d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting delta-spark==3.2.0\n",
            "  Downloading delta_spark-3.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from delta-spark==3.2.0) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.0.0->delta-spark==3.2.0) (3.23.0)\n",
            "Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: delta-spark\n",
            "Successfully installed delta-spark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"citibike\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ],
      "metadata": {
        "id": "0z96Fk-9PqAw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silver_df = spark.read.format(\"delta\").load(\"/content/silver/jc_citibike\")\n",
        "silver_df.show(10, False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "bAbLP0hHPuQY",
        "outputId": "5493febe-800b-4e9c-c1e7-f0f58991114c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o32.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1552723065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msilver_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/silver/jc_citibike\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msilver_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gold layer\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,to_date\n",
        "spark = SparkSession.builder.appName(\"citibike\").getOrCreate()\n",
        "silver_df = spark.read.format(\"delta\").load(\"/content/silver/jc_citibike\")\n",
        "silver_df.show(10, False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "0LmMRIpOOq8q",
        "outputId": "b153bd52-ec99-4229-fce0-33e88c21bc94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o26.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3748161423.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"citibike\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msilver_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/silver/jc_citibike\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msilver_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n"
          ]
        }
      ]
    }
  ]
}